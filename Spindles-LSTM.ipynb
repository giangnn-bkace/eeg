{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply LSTM model to classify Spindles Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages need to be installed\n",
    "\n",
    "```shell\n",
    "> conda install numpy pandas tensorflow-gpu scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the dataset\n",
    "DATA_PATH = \"data_excerpt1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(DATA_PATH, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the data is correctly loaded\n",
    "First 10 rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.9562</td>\n",
       "      <td>-0.8962</td>\n",
       "      <td>-3.1877</td>\n",
       "      <td>-4.1783</td>\n",
       "      <td>-5.3190</td>\n",
       "      <td>-6.1896</td>\n",
       "      <td>-7.8006</td>\n",
       "      <td>-6.9701</td>\n",
       "      <td>-0.7061</td>\n",
       "      <td>2.3058</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1483</td>\n",
       "      <td>6.0882</td>\n",
       "      <td>1.1451</td>\n",
       "      <td>-2.5673</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>6.9588</td>\n",
       "      <td>4.8775</td>\n",
       "      <td>-2.9575</td>\n",
       "      <td>-7.4704</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.8313</td>\n",
       "      <td>-8.5511</td>\n",
       "      <td>-6.5198</td>\n",
       "      <td>-4.1983</td>\n",
       "      <td>2.2958</td>\n",
       "      <td>6.1183</td>\n",
       "      <td>5.2477</td>\n",
       "      <td>3.8268</td>\n",
       "      <td>2.6561</td>\n",
       "      <td>2.7161</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6667</td>\n",
       "      <td>-1.2364</td>\n",
       "      <td>-6.6799</td>\n",
       "      <td>-4.0282</td>\n",
       "      <td>-1.8968</td>\n",
       "      <td>-4.3584</td>\n",
       "      <td>-3.7580</td>\n",
       "      <td>-0.8362</td>\n",
       "      <td>-6.4598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-12.7638</td>\n",
       "      <td>-7.7906</td>\n",
       "      <td>-5.7993</td>\n",
       "      <td>-9.7518</td>\n",
       "      <td>-5.4891</td>\n",
       "      <td>-3.3878</td>\n",
       "      <td>-5.4791</td>\n",
       "      <td>-3.9381</td>\n",
       "      <td>-4.6186</td>\n",
       "      <td>-5.0589</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.8000</td>\n",
       "      <td>-6.3097</td>\n",
       "      <td>-4.8687</td>\n",
       "      <td>-2.7074</td>\n",
       "      <td>-1.0463</td>\n",
       "      <td>-2.3972</td>\n",
       "      <td>0.6148</td>\n",
       "      <td>5.4378</td>\n",
       "      <td>4.3371</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5647</td>\n",
       "      <td>1.2752</td>\n",
       "      <td>2.1357</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>3.6467</td>\n",
       "      <td>2.2358</td>\n",
       "      <td>-2.2170</td>\n",
       "      <td>1.7355</td>\n",
       "      <td>4.7074</td>\n",
       "      <td>3.8268</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>1.3252</td>\n",
       "      <td>-3.0876</td>\n",
       "      <td>3.0463</td>\n",
       "      <td>-1.8468</td>\n",
       "      <td>-1.8168</td>\n",
       "      <td>-2.1770</td>\n",
       "      <td>-6.6098</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.9882</td>\n",
       "      <td>-5.1989</td>\n",
       "      <td>-10.8325</td>\n",
       "      <td>-11.8732</td>\n",
       "      <td>-8.7412</td>\n",
       "      <td>-7.9107</td>\n",
       "      <td>-4.7687</td>\n",
       "      <td>-4.1183</td>\n",
       "      <td>-7.0001</td>\n",
       "      <td>-5.2490</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3258</td>\n",
       "      <td>2.3459</td>\n",
       "      <td>-1.1364</td>\n",
       "      <td>-4.6386</td>\n",
       "      <td>-4.6486</td>\n",
       "      <td>-0.5560</td>\n",
       "      <td>-0.7661</td>\n",
       "      <td>-3.8881</td>\n",
       "      <td>-4.3184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-6.4698</td>\n",
       "      <td>-8.9013</td>\n",
       "      <td>-8.0708</td>\n",
       "      <td>-3.5679</td>\n",
       "      <td>-4.1683</td>\n",
       "      <td>-7.6505</td>\n",
       "      <td>-5.6793</td>\n",
       "      <td>-2.2471</td>\n",
       "      <td>1.2051</td>\n",
       "      <td>-6.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>2.3058</td>\n",
       "      <td>7.8594</td>\n",
       "      <td>3.7568</td>\n",
       "      <td>-4.2383</td>\n",
       "      <td>-2.6573</td>\n",
       "      <td>4.6873</td>\n",
       "      <td>3.2764</td>\n",
       "      <td>-0.7961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2.3271</td>\n",
       "      <td>-6.3097</td>\n",
       "      <td>-6.7699</td>\n",
       "      <td>-4.6586</td>\n",
       "      <td>-2.6573</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>3.3165</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>5.8581</td>\n",
       "      <td>5.5579</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0169</td>\n",
       "      <td>1.3152</td>\n",
       "      <td>0.5847</td>\n",
       "      <td>-2.6373</td>\n",
       "      <td>-5.8994</td>\n",
       "      <td>-4.0982</td>\n",
       "      <td>-4.9288</td>\n",
       "      <td>-10.1921</td>\n",
       "      <td>-11.4629</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.7318</td>\n",
       "      <td>-7.2903</td>\n",
       "      <td>-3.3878</td>\n",
       "      <td>1.8656</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>4.3872</td>\n",
       "      <td>12.3822</td>\n",
       "      <td>15.8545</td>\n",
       "      <td>13.8732</td>\n",
       "      <td>14.3035</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5511</td>\n",
       "      <td>11.9920</td>\n",
       "      <td>10.8112</td>\n",
       "      <td>8.6999</td>\n",
       "      <td>6.8787</td>\n",
       "      <td>2.4059</td>\n",
       "      <td>1.4653</td>\n",
       "      <td>2.7261</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.2758</td>\n",
       "      <td>7.7793</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>9.9707</td>\n",
       "      <td>4.5072</td>\n",
       "      <td>0.4747</td>\n",
       "      <td>-3.3378</td>\n",
       "      <td>-6.5798</td>\n",
       "      <td>-8.4810</td>\n",
       "      <td>-7.9307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3359</td>\n",
       "      <td>4.0569</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>-3.3578</td>\n",
       "      <td>-1.9369</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>-4.6486</td>\n",
       "      <td>-2.7674</td>\n",
       "      <td>-1.1764</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.0801</td>\n",
       "      <td>-4.2884</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>-2.1870</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>2.5960</td>\n",
       "      <td>2.9462</td>\n",
       "      <td>-1.2064</td>\n",
       "      <td>-3.0176</td>\n",
       "      <td>-0.0457</td>\n",
       "      <td>...</td>\n",
       "      <td>11.9220</td>\n",
       "      <td>9.6505</td>\n",
       "      <td>6.9088</td>\n",
       "      <td>13.2828</td>\n",
       "      <td>13.0827</td>\n",
       "      <td>12.1021</td>\n",
       "      <td>11.9420</td>\n",
       "      <td>9.2402</td>\n",
       "      <td>10.7712</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1        2        3       4       5        6        7  \\\n",
       "0  -0.9562 -0.8962  -3.1877  -4.1783 -5.3190 -6.1896  -7.8006  -6.9701   \n",
       "1  -8.8313 -8.5511  -6.5198  -4.1983  2.2958  6.1183   5.2477   3.8268   \n",
       "2 -12.7638 -7.7906  -5.7993  -9.7518 -5.4891 -3.3878  -5.4791  -3.9381   \n",
       "3   0.5647  1.2752   2.1357   0.8049  3.6467  2.2358  -2.2170   1.7355   \n",
       "4  -3.9882 -5.1989 -10.8325 -11.8732 -8.7412 -7.9107  -4.7687  -4.1183   \n",
       "5  -6.4698 -8.9013  -8.0708  -3.5679 -4.1683 -7.6505  -5.6793  -2.2471   \n",
       "6  -2.3271 -6.3097  -6.7699  -4.6586 -2.6573  1.0050   3.3165   4.7174   \n",
       "7  -9.7318 -7.2903  -3.3878   1.8656  2.9562  4.3872  12.3822  15.8545   \n",
       "8   2.2758  7.7793  12.0220   9.9707  4.5072  0.4747  -3.3378  -6.5798   \n",
       "9  -7.0801 -4.2884   1.2852  -2.1870 -0.0257  2.5960   2.9462  -1.2064   \n",
       "\n",
       "         8        9  ...      491      492      493      494      495  \\\n",
       "0  -0.7061   2.3058  ...   6.1483   6.0882   1.1451  -2.5673   0.7148   \n",
       "1   2.6561   2.7161  ...   3.6667  -1.2364  -6.6799  -4.0282  -1.8968   \n",
       "2  -4.6186  -5.0589  ...  -6.8000  -6.3097  -4.8687  -2.7074  -1.0463   \n",
       "3   4.7074   3.8268  ...   2.9562   7.6292   1.3252  -3.0876   3.0463   \n",
       "4  -7.0001  -5.2490  ...   2.3258   2.3459  -1.1364  -4.6386  -4.6486   \n",
       "5   1.2051  -6.8100  ...   0.7749   2.3058   7.8594   3.7568  -4.2383   \n",
       "6   5.8581   5.5579  ...  -2.0169   1.3152   0.5847  -2.6373  -5.8994   \n",
       "7  13.8732  14.3035  ...  10.5511  11.9920  10.8112   8.6999   6.8787   \n",
       "8  -8.4810  -7.9307  ...  -0.3359   4.0569   2.9562  -3.3578  -1.9369   \n",
       "9  -3.0176  -0.0457  ...  11.9220   9.6505   6.9088  13.2828  13.0827   \n",
       "\n",
       "       496      497      498      499  Label  \n",
       "0   6.9588   4.8775  -2.9575  -7.4704      5  \n",
       "1  -4.3584  -3.7580  -0.8362  -6.4598      5  \n",
       "2  -2.3972   0.6148   5.4378   4.3371      5  \n",
       "3  -1.8468  -1.8168  -2.1770  -6.6098      5  \n",
       "4  -0.5560  -0.7661  -3.8881  -4.3184      5  \n",
       "5  -2.6573   4.6873   3.2764  -0.7961      5  \n",
       "6  -4.0982  -4.9288 -10.1921 -11.4629      5  \n",
       "7   2.4059   1.4653   2.7261   0.7949      5  \n",
       "8  -2.9275  -4.6486  -2.7674  -1.1764      5  \n",
       "9  12.1021  11.9420   9.2402  10.7712      5  \n",
       "\n",
       "[10 rows x 501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(data.iloc[:,:-1])\n",
    "y = np.asarray(data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate training data and test data\n",
    "\n",
    "Using cross_validation functions from Scikit-learn package\n",
    "\n",
    "For simple, I used train_test_split to split the data (not use k-fold cross-vaildation yet).\n",
    "I splitted 1/4 of the data as test data (because there are only 4 samples of class 0)\n",
    "\n",
    "For other cross-validation methods, please check this link: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270, 500), (90, 500), (270,), (90,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant for create model\n",
    "Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = x.shape[1]\n",
    "NUM_CLASSES = len(np.unique(y))\n",
    "NUM_TRAIN_SAMPLE = y_train.shape[0]\n",
    "NUM_TEST_SAMPLE = y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "A simple model with only 1 LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes in the LSTM layer\n",
    "# You can change this\n",
    "LSTM_SIZE = 10\n",
    "\n",
    "# Dropout probability\n",
    "# You can change it in range [0,1]\n",
    "DROPOUT = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 55        \n",
      "=================================================================\n",
      "Total params: 535\n",
      "Trainable params: 535\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((NUM_FEATURES, 1), input_shape=(NUM_FEATURES,)))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=False, input_shape=(NUM_FEATURES, 1)))\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "# you can change this\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# number of epochs to train the model\n",
    "# you can change this\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# do not change this\n",
    "PER_EPOCH_STEPS = NUM_TRAIN_SAMPLE//BATCH_SIZE\n",
    "TEST_PER_EPOCH_STEPS = NUM_TEST_SAMPLE//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensorflow data to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 2 1 5 2 5 1 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train[:10])\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "y_new = encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_new[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class labels as one-hot vectors\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = encoder.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(NUM_TRAIN_SAMPLE)\n",
    "train_dataset = train_dataset.repeat().batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the optimizer to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# you can train this\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Using Adam optimizer\n",
    "# and categorical_crossentropy as loss function\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(LEARNING_RATE), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 1.6822 - acc: 0.2148 - val_loss: 1.6469 - val_acc: 0.2889\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 26s 971ms/step - loss: 1.5374 - acc: 0.3556 - val_loss: 1.5007 - val_acc: 0.3667\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 26s 972ms/step - loss: 1.4158 - acc: 0.4259 - val_loss: 1.3760 - val_acc: 0.4000\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 1.3144 - acc: 0.4815 - val_loss: 1.2887 - val_acc: 0.6000\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 26s 980ms/step - loss: 1.2359 - acc: 0.6037 - val_loss: 1.2277 - val_acc: 0.6000\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 1.1811 - acc: 0.6037 - val_loss: 1.1868 - val_acc: 0.6111\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 26s 976ms/step - loss: 1.1426 - acc: 0.6074 - val_loss: 1.1563 - val_acc: 0.6111\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 1.1136 - acc: 0.6111 - val_loss: 1.1354 - val_acc: 0.6111\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 26s 965ms/step - loss: 1.0920 - acc: 0.6111 - val_loss: 1.1192 - val_acc: 0.6111\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 26s 953ms/step - loss: 1.0756 - acc: 0.6111 - val_loss: 1.1058 - val_acc: 0.6111\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 26s 954ms/step - loss: 1.0631 - acc: 0.6111 - val_loss: 1.0928 - val_acc: 0.6111\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 26s 954ms/step - loss: 1.0507 - acc: 0.6111 - val_loss: 1.0857 - val_acc: 0.6111\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 1.0412 - acc: 0.6111 - val_loss: 1.0761 - val_acc: 0.6111\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 26s 950ms/step - loss: 1.0336 - acc: 0.6111 - val_loss: 1.0702 - val_acc: 0.6111\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 1.0257 - acc: 0.6148 - val_loss: 1.0629 - val_acc: 0.6111\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 1.0184 - acc: 0.6148 - val_loss: 1.0575 - val_acc: 0.6111\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 26s 960ms/step - loss: 1.0130 - acc: 0.6222 - val_loss: 1.0479 - val_acc: 0.6111\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.9998 - acc: 0.6259 - val_loss: 1.0403 - val_acc: 0.6111\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.9894 - acc: 0.6259 - val_loss: 1.0291 - val_acc: 0.6111\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 26s 954ms/step - loss: 0.9823 - acc: 0.6259 - val_loss: 1.0197 - val_acc: 0.6111\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 26s 973ms/step - loss: 0.9708 - acc: 0.6296 - val_loss: 1.0105 - val_acc: 0.6222\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 26s 957ms/step - loss: 0.9638 - acc: 0.6370 - val_loss: 1.0025 - val_acc: 0.6222\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.9494 - acc: 0.6519 - val_loss: 1.0011 - val_acc: 0.6222\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.9360 - acc: 0.6481 - val_loss: 0.9843 - val_acc: 0.6333\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.9376 - acc: 0.6444 - val_loss: 0.9827 - val_acc: 0.6222\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.9142 - acc: 0.6667 - val_loss: 0.9723 - val_acc: 0.6222\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.9062 - acc: 0.6667 - val_loss: 0.9803 - val_acc: 0.6444\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 26s 966ms/step - loss: 0.8812 - acc: 0.6704 - val_loss: 0.9380 - val_acc: 0.6556\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 26s 962ms/step - loss: 0.9261 - acc: 0.6704 - val_loss: 0.9816 - val_acc: 0.6333\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 26s 961ms/step - loss: 0.9014 - acc: 0.6704 - val_loss: 0.9050 - val_acc: 0.6778\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.8404 - acc: 0.6852 - val_loss: 0.9367 - val_acc: 0.6556\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.8410 - acc: 0.6815 - val_loss: 0.9408 - val_acc: 0.6556\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.9378 - acc: 0.6259 - val_loss: 0.9081 - val_acc: 0.6667\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 26s 963ms/step - loss: 0.8723 - acc: 0.6593 - val_loss: 1.0246 - val_acc: 0.6111\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 26s 961ms/step - loss: 0.8663 - acc: 0.6593 - val_loss: 0.9138 - val_acc: 0.6556\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 26s 963ms/step - loss: 0.8144 - acc: 0.6889 - val_loss: 0.9339 - val_acc: 0.6444\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.7987 - acc: 0.7000 - val_loss: 0.9118 - val_acc: 0.6556\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.8060 - acc: 0.6852 - val_loss: 0.9082 - val_acc: 0.6667\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 26s 960ms/step - loss: 0.8124 - acc: 0.6852 - val_loss: 0.9670 - val_acc: 0.6556\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 26s 954ms/step - loss: 0.8008 - acc: 0.7037 - val_loss: 0.8679 - val_acc: 0.6889\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.7860 - acc: 0.6926 - val_loss: 0.8653 - val_acc: 0.6889\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.7733 - acc: 0.7000 - val_loss: 0.8679 - val_acc: 0.6889\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 26s 963ms/step - loss: 0.7719 - acc: 0.7037 - val_loss: 0.9567 - val_acc: 0.6667\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.7918 - acc: 0.7000 - val_loss: 1.0984 - val_acc: 0.5444\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.8157 - acc: 0.6519 - val_loss: 0.9239 - val_acc: 0.6556\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 26s 970ms/step - loss: 0.7922 - acc: 0.6704 - val_loss: 0.9492 - val_acc: 0.6556\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.8021 - acc: 0.6741 - val_loss: 0.9013 - val_acc: 0.6778\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 26s 963ms/step - loss: 0.8196 - acc: 0.6481 - val_loss: 0.9579 - val_acc: 0.6333\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 26s 967ms/step - loss: 0.7815 - acc: 0.6741 - val_loss: 0.9044 - val_acc: 0.6667\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.7537 - acc: 0.6889 - val_loss: 0.9528 - val_acc: 0.6444\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 26s 974ms/step - loss: 0.7708 - acc: 0.6704 - val_loss: 0.9037 - val_acc: 0.6556\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 26s 977ms/step - loss: 0.7610 - acc: 0.7000 - val_loss: 0.9232 - val_acc: 0.6556\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 27s 991ms/step - loss: 0.7633 - acc: 0.7000 - val_loss: 0.9256 - val_acc: 0.6667\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 26s 968ms/step - loss: 0.7542 - acc: 0.6963 - val_loss: 0.9218 - val_acc: 0.6667\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 26s 968ms/step - loss: 0.7784 - acc: 0.6852 - val_loss: 1.3487 - val_acc: 0.4333\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 26s 962ms/step - loss: 1.1345 - acc: 0.4963 - val_loss: 1.1159 - val_acc: 0.5556\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.7919 - acc: 0.6667 - val_loss: 0.9428 - val_acc: 0.6667\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.7487 - acc: 0.7000 - val_loss: 0.9560 - val_acc: 0.6667\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.7806 - acc: 0.6889 - val_loss: 0.9736 - val_acc: 0.6444\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 26s 968ms/step - loss: 0.7674 - acc: 0.6963 - val_loss: 0.9660 - val_acc: 0.6333\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 26s 969ms/step - loss: 0.7713 - acc: 0.6889 - val_loss: 0.9500 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "27/27 [==============================] - 26s 969ms/step - loss: 0.7451 - acc: 0.7000 - val_loss: 0.9517 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.7757 - acc: 0.6778 - val_loss: 0.9651 - val_acc: 0.6556\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.7641 - acc: 0.6852 - val_loss: 0.9573 - val_acc: 0.6444\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.7611 - acc: 0.7000 - val_loss: 0.9708 - val_acc: 0.6556\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 26s 953ms/step - loss: 0.7884 - acc: 0.7148 - val_loss: 0.9626 - val_acc: 0.6667\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.7432 - acc: 0.6963 - val_loss: 0.8784 - val_acc: 0.6889\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 26s 968ms/step - loss: 0.7178 - acc: 0.7222 - val_loss: 0.8831 - val_acc: 0.6889\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.7235 - acc: 0.7148 - val_loss: 0.8828 - val_acc: 0.6778\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 26s 957ms/step - loss: 0.7351 - acc: 0.7148 - val_loss: 0.8705 - val_acc: 0.6889\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.7148 - acc: 0.7074 - val_loss: 0.8655 - val_acc: 0.6889\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 26s 956ms/step - loss: 0.7114 - acc: 0.7148 - val_loss: 0.8687 - val_acc: 0.6889\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 26s 953ms/step - loss: 0.7073 - acc: 0.7259 - val_loss: 0.8608 - val_acc: 0.6889\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 26s 960ms/step - loss: 0.7067 - acc: 0.7111 - val_loss: 0.8734 - val_acc: 0.6889\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 26s 961ms/step - loss: 0.7047 - acc: 0.7148 - val_loss: 0.8567 - val_acc: 0.6778\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 26s 974ms/step - loss: 0.7002 - acc: 0.7259 - val_loss: 0.8567 - val_acc: 0.6778\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.6967 - acc: 0.7222 - val_loss: 0.8628 - val_acc: 0.6667\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.6961 - acc: 0.7222 - val_loss: 0.8604 - val_acc: 0.6667\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 26s 970ms/step - loss: 0.6930 - acc: 0.7185 - val_loss: 0.8936 - val_acc: 0.6556\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 26s 972ms/step - loss: 0.6927 - acc: 0.7222 - val_loss: 0.8921 - val_acc: 0.6444\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 27s 989ms/step - loss: 0.7011 - acc: 0.7259 - val_loss: 0.8666 - val_acc: 0.6444\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 26s 953ms/step - loss: 0.6796 - acc: 0.7333 - val_loss: 0.9004 - val_acc: 0.6556\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.6731 - acc: 0.7333 - val_loss: 0.8993 - val_acc: 0.6444\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 26s 959ms/step - loss: 0.6679 - acc: 0.7333 - val_loss: 0.9014 - val_acc: 0.6333\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 26s 957ms/step - loss: 0.6700 - acc: 0.7333 - val_loss: 0.8662 - val_acc: 0.6778\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 26s 967ms/step - loss: 0.6689 - acc: 0.7407 - val_loss: 0.8967 - val_acc: 0.6444\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 26s 970ms/step - loss: 0.6741 - acc: 0.7333 - val_loss: 0.8926 - val_acc: 0.6556\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 26s 975ms/step - loss: 0.6773 - acc: 0.7407 - val_loss: 0.8847 - val_acc: 0.6556\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 26s 965ms/step - loss: 0.6657 - acc: 0.7407 - val_loss: 0.8690 - val_acc: 0.6222\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 26s 958ms/step - loss: 0.7221 - acc: 0.7185 - val_loss: 0.9500 - val_acc: 0.6222\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 0.7687 - acc: 0.7111 - val_loss: 0.9835 - val_acc: 0.6222\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.8416 - acc: 0.6926 - val_loss: 1.0400 - val_acc: 0.5444\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 26s 974ms/step - loss: 0.6973 - acc: 0.7222 - val_loss: 0.9277 - val_acc: 0.6333\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 27s 991ms/step - loss: 0.6658 - acc: 0.7333 - val_loss: 0.9266 - val_acc: 0.6444\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.6648 - acc: 0.7333 - val_loss: 0.8977 - val_acc: 0.6111\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 26s 957ms/step - loss: 0.6628 - acc: 0.7444 - val_loss: 0.9221 - val_acc: 0.6333\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.6551 - acc: 0.7407 - val_loss: 0.9250 - val_acc: 0.6333\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 26s 955ms/step - loss: 0.6480 - acc: 0.7444 - val_loss: 0.9251 - val_acc: 0.6444\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 26s 962ms/step - loss: 0.7152 - acc: 0.7370 - val_loss: 0.9025 - val_acc: 0.6444\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 26s 948ms/step - loss: 0.6660 - acc: 0.7333 - val_loss: 0.8734 - val_acc: 0.6778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2317d457a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          steps_per_epoch=PER_EPOCH_STEPS, \n",
    "          validation_data=test_dataset, \n",
    "          validation_steps=TEST_PER_EPOCH_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wave",
   "language": "python",
   "name": "wave"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
