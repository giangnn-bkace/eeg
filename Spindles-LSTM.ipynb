{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply LSTM model to classify Spindles Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages need to be installed\n",
    "\n",
    "```shell\n",
    "> conda install numpy pandas tensorflow-gpu scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the dataset\n",
    "DATA_PATH = \"data_excerpt1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(DATA_PATH, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the data is correctly loaded\n",
    "First 10 rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.9562</td>\n",
       "      <td>-0.8962</td>\n",
       "      <td>-3.1877</td>\n",
       "      <td>-4.1783</td>\n",
       "      <td>-5.3190</td>\n",
       "      <td>-6.1896</td>\n",
       "      <td>-7.8006</td>\n",
       "      <td>-6.9701</td>\n",
       "      <td>-0.7061</td>\n",
       "      <td>2.3058</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1483</td>\n",
       "      <td>6.0882</td>\n",
       "      <td>1.1451</td>\n",
       "      <td>-2.5673</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>6.9588</td>\n",
       "      <td>4.8775</td>\n",
       "      <td>-2.9575</td>\n",
       "      <td>-7.4704</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-8.8313</td>\n",
       "      <td>-8.5511</td>\n",
       "      <td>-6.5198</td>\n",
       "      <td>-4.1983</td>\n",
       "      <td>2.2958</td>\n",
       "      <td>6.1183</td>\n",
       "      <td>5.2477</td>\n",
       "      <td>3.8268</td>\n",
       "      <td>2.6561</td>\n",
       "      <td>2.7161</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6667</td>\n",
       "      <td>-1.2364</td>\n",
       "      <td>-6.6799</td>\n",
       "      <td>-4.0282</td>\n",
       "      <td>-1.8968</td>\n",
       "      <td>-4.3584</td>\n",
       "      <td>-3.7580</td>\n",
       "      <td>-0.8362</td>\n",
       "      <td>-6.4598</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-12.7638</td>\n",
       "      <td>-7.7906</td>\n",
       "      <td>-5.7993</td>\n",
       "      <td>-9.7518</td>\n",
       "      <td>-5.4891</td>\n",
       "      <td>-3.3878</td>\n",
       "      <td>-5.4791</td>\n",
       "      <td>-3.9381</td>\n",
       "      <td>-4.6186</td>\n",
       "      <td>-5.0589</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.8000</td>\n",
       "      <td>-6.3097</td>\n",
       "      <td>-4.8687</td>\n",
       "      <td>-2.7074</td>\n",
       "      <td>-1.0463</td>\n",
       "      <td>-2.3972</td>\n",
       "      <td>0.6148</td>\n",
       "      <td>5.4378</td>\n",
       "      <td>4.3371</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5647</td>\n",
       "      <td>1.2752</td>\n",
       "      <td>2.1357</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>3.6467</td>\n",
       "      <td>2.2358</td>\n",
       "      <td>-2.2170</td>\n",
       "      <td>1.7355</td>\n",
       "      <td>4.7074</td>\n",
       "      <td>3.8268</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>7.6292</td>\n",
       "      <td>1.3252</td>\n",
       "      <td>-3.0876</td>\n",
       "      <td>3.0463</td>\n",
       "      <td>-1.8468</td>\n",
       "      <td>-1.8168</td>\n",
       "      <td>-2.1770</td>\n",
       "      <td>-6.6098</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.9882</td>\n",
       "      <td>-5.1989</td>\n",
       "      <td>-10.8325</td>\n",
       "      <td>-11.8732</td>\n",
       "      <td>-8.7412</td>\n",
       "      <td>-7.9107</td>\n",
       "      <td>-4.7687</td>\n",
       "      <td>-4.1183</td>\n",
       "      <td>-7.0001</td>\n",
       "      <td>-5.2490</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3258</td>\n",
       "      <td>2.3459</td>\n",
       "      <td>-1.1364</td>\n",
       "      <td>-4.6386</td>\n",
       "      <td>-4.6486</td>\n",
       "      <td>-0.5560</td>\n",
       "      <td>-0.7661</td>\n",
       "      <td>-3.8881</td>\n",
       "      <td>-4.3184</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-6.4698</td>\n",
       "      <td>-8.9013</td>\n",
       "      <td>-8.0708</td>\n",
       "      <td>-3.5679</td>\n",
       "      <td>-4.1683</td>\n",
       "      <td>-7.6505</td>\n",
       "      <td>-5.6793</td>\n",
       "      <td>-2.2471</td>\n",
       "      <td>1.2051</td>\n",
       "      <td>-6.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7749</td>\n",
       "      <td>2.3058</td>\n",
       "      <td>7.8594</td>\n",
       "      <td>3.7568</td>\n",
       "      <td>-4.2383</td>\n",
       "      <td>-2.6573</td>\n",
       "      <td>4.6873</td>\n",
       "      <td>3.2764</td>\n",
       "      <td>-0.7961</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2.3271</td>\n",
       "      <td>-6.3097</td>\n",
       "      <td>-6.7699</td>\n",
       "      <td>-4.6586</td>\n",
       "      <td>-2.6573</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>3.3165</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>5.8581</td>\n",
       "      <td>5.5579</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0169</td>\n",
       "      <td>1.3152</td>\n",
       "      <td>0.5847</td>\n",
       "      <td>-2.6373</td>\n",
       "      <td>-5.8994</td>\n",
       "      <td>-4.0982</td>\n",
       "      <td>-4.9288</td>\n",
       "      <td>-10.1921</td>\n",
       "      <td>-11.4629</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-9.7318</td>\n",
       "      <td>-7.2903</td>\n",
       "      <td>-3.3878</td>\n",
       "      <td>1.8656</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>4.3872</td>\n",
       "      <td>12.3822</td>\n",
       "      <td>15.8545</td>\n",
       "      <td>13.8732</td>\n",
       "      <td>14.3035</td>\n",
       "      <td>...</td>\n",
       "      <td>10.5511</td>\n",
       "      <td>11.9920</td>\n",
       "      <td>10.8112</td>\n",
       "      <td>8.6999</td>\n",
       "      <td>6.8787</td>\n",
       "      <td>2.4059</td>\n",
       "      <td>1.4653</td>\n",
       "      <td>2.7261</td>\n",
       "      <td>0.7949</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.2758</td>\n",
       "      <td>7.7793</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>9.9707</td>\n",
       "      <td>4.5072</td>\n",
       "      <td>0.4747</td>\n",
       "      <td>-3.3378</td>\n",
       "      <td>-6.5798</td>\n",
       "      <td>-8.4810</td>\n",
       "      <td>-7.9307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3359</td>\n",
       "      <td>4.0569</td>\n",
       "      <td>2.9562</td>\n",
       "      <td>-3.3578</td>\n",
       "      <td>-1.9369</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>-4.6486</td>\n",
       "      <td>-2.7674</td>\n",
       "      <td>-1.1764</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-7.0801</td>\n",
       "      <td>-4.2884</td>\n",
       "      <td>1.2852</td>\n",
       "      <td>-2.1870</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>2.5960</td>\n",
       "      <td>2.9462</td>\n",
       "      <td>-1.2064</td>\n",
       "      <td>-3.0176</td>\n",
       "      <td>-0.0457</td>\n",
       "      <td>...</td>\n",
       "      <td>11.9220</td>\n",
       "      <td>9.6505</td>\n",
       "      <td>6.9088</td>\n",
       "      <td>13.2828</td>\n",
       "      <td>13.0827</td>\n",
       "      <td>12.1021</td>\n",
       "      <td>11.9420</td>\n",
       "      <td>9.2402</td>\n",
       "      <td>10.7712</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1        2        3       4       5        6        7  \\\n",
       "0  -0.9562 -0.8962  -3.1877  -4.1783 -5.3190 -6.1896  -7.8006  -6.9701   \n",
       "1  -8.8313 -8.5511  -6.5198  -4.1983  2.2958  6.1183   5.2477   3.8268   \n",
       "2 -12.7638 -7.7906  -5.7993  -9.7518 -5.4891 -3.3878  -5.4791  -3.9381   \n",
       "3   0.5647  1.2752   2.1357   0.8049  3.6467  2.2358  -2.2170   1.7355   \n",
       "4  -3.9882 -5.1989 -10.8325 -11.8732 -8.7412 -7.9107  -4.7687  -4.1183   \n",
       "5  -6.4698 -8.9013  -8.0708  -3.5679 -4.1683 -7.6505  -5.6793  -2.2471   \n",
       "6  -2.3271 -6.3097  -6.7699  -4.6586 -2.6573  1.0050   3.3165   4.7174   \n",
       "7  -9.7318 -7.2903  -3.3878   1.8656  2.9562  4.3872  12.3822  15.8545   \n",
       "8   2.2758  7.7793  12.0220   9.9707  4.5072  0.4747  -3.3378  -6.5798   \n",
       "9  -7.0801 -4.2884   1.2852  -2.1870 -0.0257  2.5960   2.9462  -1.2064   \n",
       "\n",
       "         8        9  ...      491      492      493      494      495  \\\n",
       "0  -0.7061   2.3058  ...   6.1483   6.0882   1.1451  -2.5673   0.7148   \n",
       "1   2.6561   2.7161  ...   3.6667  -1.2364  -6.6799  -4.0282  -1.8968   \n",
       "2  -4.6186  -5.0589  ...  -6.8000  -6.3097  -4.8687  -2.7074  -1.0463   \n",
       "3   4.7074   3.8268  ...   2.9562   7.6292   1.3252  -3.0876   3.0463   \n",
       "4  -7.0001  -5.2490  ...   2.3258   2.3459  -1.1364  -4.6386  -4.6486   \n",
       "5   1.2051  -6.8100  ...   0.7749   2.3058   7.8594   3.7568  -4.2383   \n",
       "6   5.8581   5.5579  ...  -2.0169   1.3152   0.5847  -2.6373  -5.8994   \n",
       "7  13.8732  14.3035  ...  10.5511  11.9920  10.8112   8.6999   6.8787   \n",
       "8  -8.4810  -7.9307  ...  -0.3359   4.0569   2.9562  -3.3578  -1.9369   \n",
       "9  -3.0176  -0.0457  ...  11.9220   9.6505   6.9088  13.2828  13.0827   \n",
       "\n",
       "       496      497      498      499  Label  \n",
       "0   6.9588   4.8775  -2.9575  -7.4704      5  \n",
       "1  -4.3584  -3.7580  -0.8362  -6.4598      5  \n",
       "2  -2.3972   0.6148   5.4378   4.3371      5  \n",
       "3  -1.8468  -1.8168  -2.1770  -6.6098      5  \n",
       "4  -0.5560  -0.7661  -3.8881  -4.3184      5  \n",
       "5  -2.6573   4.6873   3.2764  -0.7961      5  \n",
       "6  -4.0982  -4.9288 -10.1921 -11.4629      5  \n",
       "7   2.4059   1.4653   2.7261   0.7949      5  \n",
       "8  -2.9275  -4.6486  -2.7674  -1.1764      5  \n",
       "9  12.1021  11.9420   9.2402  10.7712      5  \n",
       "\n",
       "[10 rows x 501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(data.iloc[:,:-1])\n",
    "y = np.asarray(data.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate training data and test data\n",
    "\n",
    "Using cross_validation functions from Scikit-learn package\n",
    "\n",
    "For simple, I used train_test_split to split the data (not use k-fold cross-vaildation yet).\n",
    "I splitted 1/4 of the data as test data (because there are only 4 samples of class 0)\n",
    "\n",
    "For other cross-validation methods, please check this link: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270, 500), (90, 500), (270,), (90,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant for create model\n",
    "Do not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = x.shape[1]\n",
    "NUM_CLASSES = len(np.unique(y))\n",
    "NUM_TRAIN_SAMPLE = y_train.shape[0]\n",
    "NUM_TEST_SAMPLE = y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "A simple model with only 1 LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes in the LSTM layer\n",
    "# You can change this\n",
    "LSTM_SIZE = 50\n",
    "\n",
    "# Dropout probability\n",
    "# You can change it in range [0,1]\n",
    "DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 500, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 10,655\n",
      "Trainable params: 10,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((NUM_FEATURES, 1), input_shape=(NUM_FEATURES,)))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_SIZE, return_sequences=False, input_shape=(NUM_FEATURES, 1)))\n",
    "model.add(tf.keras.layers.Dropout(DROPOUT))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "# you can change this\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "# number of epochs to train the model\n",
    "# you can change this\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# do not change this\n",
    "PER_EPOCH_STEPS = NUM_TRAIN_SAMPLE//BATCH_SIZE\n",
    "TEST_PER_EPOCH_STEPS = NUM_TEST_SAMPLE//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tensorflow data to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class labels as one-hot vectors\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = encoder.fit_transform(y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(NUM_TRAIN_SAMPLE)\n",
    "train_dataset = train_dataset.repeat().batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the optimizer to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "# you can train this\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Using Adam optimizer\n",
    "# and categorical_crossentropy as loss function\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(LEARNING_RATE), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 28s 1s/step - loss: 1.3550 - acc: 0.5037 - val_loss: 1.1651 - val_acc: 0.6111\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 26s 980ms/step - loss: 1.0836 - acc: 0.6111 - val_loss: 1.1130 - val_acc: 0.6111\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 1.0438 - acc: 0.6185 - val_loss: 1.1033 - val_acc: 0.6111\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.9935 - acc: 0.6259 - val_loss: 1.0843 - val_acc: 0.6111\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.9655 - acc: 0.6370 - val_loss: 1.0856 - val_acc: 0.6111\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.9481 - acc: 0.6333 - val_loss: 1.0597 - val_acc: 0.6222\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 27s 987ms/step - loss: 0.9000 - acc: 0.6259 - val_loss: 1.0280 - val_acc: 0.6111\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.8647 - acc: 0.6519 - val_loss: 1.0102 - val_acc: 0.6333\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 27s 985ms/step - loss: 0.8468 - acc: 0.6704 - val_loss: 0.9873 - val_acc: 0.5778\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.8007 - acc: 0.6963 - val_loss: 0.9469 - val_acc: 0.6222\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 26s 977ms/step - loss: 0.7708 - acc: 0.6926 - val_loss: 0.9540 - val_acc: 0.6111\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 27s 993ms/step - loss: 0.7475 - acc: 0.7222 - val_loss: 0.9317 - val_acc: 0.6222\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.7275 - acc: 0.7111 - val_loss: 0.9431 - val_acc: 0.6333\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.7068 - acc: 0.7296 - val_loss: 0.9323 - val_acc: 0.6111\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.6989 - acc: 0.7370 - val_loss: 0.9223 - val_acc: 0.6222\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.7060 - acc: 0.7037 - val_loss: 0.9840 - val_acc: 0.6333\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 27s 986ms/step - loss: 0.6907 - acc: 0.6926 - val_loss: 1.0121 - val_acc: 0.6111\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 26s 980ms/step - loss: 0.6868 - acc: 0.7185 - val_loss: 0.9650 - val_acc: 0.6556\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.6469 - acc: 0.7407 - val_loss: 0.9811 - val_acc: 0.6444\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.6876 - acc: 0.7222 - val_loss: 1.1135 - val_acc: 0.6000\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.8723 - acc: 0.6852 - val_loss: 0.9945 - val_acc: 0.6556\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 27s 986ms/step - loss: 0.6794 - acc: 0.7259 - val_loss: 0.9191 - val_acc: 0.6444\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.7344 - acc: 0.7222 - val_loss: 1.1958 - val_acc: 0.5667\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.7969 - acc: 0.6630 - val_loss: 0.9765 - val_acc: 0.6000\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 27s 992ms/step - loss: 0.6727 - acc: 0.7333 - val_loss: 0.9490 - val_acc: 0.6000\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.6253 - acc: 0.7519 - val_loss: 0.9887 - val_acc: 0.6222\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 27s 985ms/step - loss: 0.6324 - acc: 0.7556 - val_loss: 0.9860 - val_acc: 0.6222\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 0.6042 - acc: 0.7815 - val_loss: 0.9586 - val_acc: 0.6444\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 27s 985ms/step - loss: 0.5947 - acc: 0.7741 - val_loss: 0.9979 - val_acc: 0.6222\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.5913 - acc: 0.7815 - val_loss: 1.0349 - val_acc: 0.5667\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 26s 980ms/step - loss: 0.5832 - acc: 0.7815 - val_loss: 1.0278 - val_acc: 0.6333\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 27s 986ms/step - loss: 0.5214 - acc: 0.8037 - val_loss: 0.9968 - val_acc: 0.6111\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 0.5423 - acc: 0.7889 - val_loss: 1.0245 - val_acc: 0.5667\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 27s 987ms/step - loss: 0.5550 - acc: 0.7852 - val_loss: 1.0797 - val_acc: 0.6444\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 27s 989ms/step - loss: 0.7497 - acc: 0.7667 - val_loss: 1.1094 - val_acc: 0.6222\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.6293 - acc: 0.7741 - val_loss: 1.1215 - val_acc: 0.6222\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 27s 997ms/step - loss: 0.6444 - acc: 0.7593 - val_loss: 1.0011 - val_acc: 0.6778\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.5803 - acc: 0.7926 - val_loss: 1.0710 - val_acc: 0.6556\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 27s 985ms/step - loss: 0.5917 - acc: 0.7778 - val_loss: 1.0459 - val_acc: 0.6111\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.6071 - acc: 0.7704 - val_loss: 1.0609 - val_acc: 0.6667\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.5491 - acc: 0.8074 - val_loss: 1.0315 - val_acc: 0.6444\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.5493 - acc: 0.8037 - val_loss: 1.0477 - val_acc: 0.6667\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 27s 983ms/step - loss: 0.5109 - acc: 0.8333 - val_loss: 1.1548 - val_acc: 0.6222\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 27s 989ms/step - loss: 0.5935 - acc: 0.7926 - val_loss: 1.1591 - val_acc: 0.5556\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.5350 - acc: 0.7815 - val_loss: 1.1654 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.6133 - acc: 0.7556 - val_loss: 1.1061 - val_acc: 0.5667\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 27s 982ms/step - loss: 0.5665 - acc: 0.7704 - val_loss: 1.2045 - val_acc: 0.6333\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.6234 - acc: 0.7370 - val_loss: 1.1081 - val_acc: 0.5778\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 28s 1s/step - loss: 0.5662 - acc: 0.7852 - val_loss: 1.0784 - val_acc: 0.6222\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.5704 - acc: 0.7815 - val_loss: 1.2187 - val_acc: 0.6444\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.6047 - acc: 0.7481 - val_loss: 1.1591 - val_acc: 0.5889\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 26s 976ms/step - loss: 0.5256 - acc: 0.7778 - val_loss: 1.1656 - val_acc: 0.6000\n",
      "Epoch 53/100\n",
      "27/27 [==============================] - 27s 994ms/step - loss: 0.5168 - acc: 0.8185 - val_loss: 1.1155 - val_acc: 0.6000\n",
      "Epoch 54/100\n",
      "27/27 [==============================] - 27s 989ms/step - loss: 0.4879 - acc: 0.7889 - val_loss: 1.1154 - val_acc: 0.6111\n",
      "Epoch 55/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.5492 - acc: 0.7593 - val_loss: 1.1075 - val_acc: 0.6333\n",
      "Epoch 56/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.5170 - acc: 0.7889 - val_loss: 1.2661 - val_acc: 0.6556\n",
      "Epoch 57/100\n",
      "27/27 [==============================] - 27s 997ms/step - loss: 0.4475 - acc: 0.8111 - val_loss: 1.1819 - val_acc: 0.6222\n",
      "Epoch 58/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.4136 - acc: 0.8370 - val_loss: 1.2026 - val_acc: 0.6111\n",
      "Epoch 59/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3972 - acc: 0.8778 - val_loss: 1.2123 - val_acc: 0.6444\n",
      "Epoch 60/100\n",
      "27/27 [==============================] - 27s 994ms/step - loss: 0.4181 - acc: 0.8333 - val_loss: 1.1755 - val_acc: 0.5889\n",
      "Epoch 61/100\n",
      "27/27 [==============================] - 27s 985ms/step - loss: 0.3819 - acc: 0.8667 - val_loss: 1.3201 - val_acc: 0.6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.4450 - acc: 0.8407 - val_loss: 1.1899 - val_acc: 0.6667\n",
      "Epoch 63/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.4003 - acc: 0.8704 - val_loss: 1.1582 - val_acc: 0.7000\n",
      "Epoch 64/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3635 - acc: 0.9000 - val_loss: 1.1537 - val_acc: 0.6111\n",
      "Epoch 65/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3999 - acc: 0.8519 - val_loss: 1.2054 - val_acc: 0.6556\n",
      "Epoch 66/100\n",
      "27/27 [==============================] - 28s 1s/step - loss: 0.3364 - acc: 0.8889 - val_loss: 1.2562 - val_acc: 0.6333\n",
      "Epoch 67/100\n",
      "27/27 [==============================] - 26s 978ms/step - loss: 0.3134 - acc: 0.8815 - val_loss: 1.2353 - val_acc: 0.6667\n",
      "Epoch 68/100\n",
      "27/27 [==============================] - 26s 980ms/step - loss: 0.3217 - acc: 0.8963 - val_loss: 1.3139 - val_acc: 0.6444\n",
      "Epoch 69/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3209 - acc: 0.9074 - val_loss: 1.3360 - val_acc: 0.6556\n",
      "Epoch 70/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3035 - acc: 0.9037 - val_loss: 1.3114 - val_acc: 0.6778\n",
      "Epoch 71/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.3066 - acc: 0.9148 - val_loss: 1.2066 - val_acc: 0.7000\n",
      "Epoch 72/100\n",
      "27/27 [==============================] - 27s 999ms/step - loss: 0.2702 - acc: 0.9444 - val_loss: 1.2072 - val_acc: 0.6556\n",
      "Epoch 73/100\n",
      "27/27 [==============================] - 27s 984ms/step - loss: 0.2831 - acc: 0.9111 - val_loss: 1.3004 - val_acc: 0.6667\n",
      "Epoch 74/100\n",
      "27/27 [==============================] - 28s 1s/step - loss: 0.2814 - acc: 0.8963 - val_loss: 1.2384 - val_acc: 0.6667\n",
      "Epoch 75/100\n",
      "27/27 [==============================] - 27s 986ms/step - loss: 0.2526 - acc: 0.9148 - val_loss: 1.2442 - val_acc: 0.6333\n",
      "Epoch 76/100\n",
      "27/27 [==============================] - 26s 977ms/step - loss: 0.2441 - acc: 0.9407 - val_loss: 1.2686 - val_acc: 0.6556\n",
      "Epoch 77/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.2330 - acc: 0.9222 - val_loss: 1.3255 - val_acc: 0.6667\n",
      "Epoch 78/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.2192 - acc: 0.9481 - val_loss: 1.3329 - val_acc: 0.6667\n",
      "Epoch 79/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2624 - acc: 0.9185 - val_loss: 1.2719 - val_acc: 0.6667\n",
      "Epoch 80/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2007 - acc: 0.9407 - val_loss: 1.4280 - val_acc: 0.6333\n",
      "Epoch 81/100\n",
      "27/27 [==============================] - 26s 976ms/step - loss: 0.2132 - acc: 0.9333 - val_loss: 1.3703 - val_acc: 0.6778\n",
      "Epoch 82/100\n",
      "27/27 [==============================] - 26s 974ms/step - loss: 0.1885 - acc: 0.9370 - val_loss: 1.4318 - val_acc: 0.6444\n",
      "Epoch 83/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.1855 - acc: 0.9519 - val_loss: 1.4400 - val_acc: 0.6889\n",
      "Epoch 84/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.1933 - acc: 0.9556 - val_loss: 1.4360 - val_acc: 0.6111\n",
      "Epoch 85/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2040 - acc: 0.9556 - val_loss: 1.2357 - val_acc: 0.6667\n",
      "Epoch 86/100\n",
      "27/27 [==============================] - 26s 973ms/step - loss: 0.1885 - acc: 0.9481 - val_loss: 1.4466 - val_acc: 0.6556\n",
      "Epoch 87/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.1876 - acc: 0.9333 - val_loss: 1.4439 - val_acc: 0.6444\n",
      "Epoch 88/100\n",
      "27/27 [==============================] - 26s 979ms/step - loss: 0.4000 - acc: 0.8519 - val_loss: 1.4625 - val_acc: 0.6333\n",
      "Epoch 89/100\n",
      "27/27 [==============================] - 26s 972ms/step - loss: 0.2775 - acc: 0.9074 - val_loss: 1.4765 - val_acc: 0.6333\n",
      "Epoch 90/100\n",
      "27/27 [==============================] - 26s 975ms/step - loss: 0.2145 - acc: 0.9148 - val_loss: 1.4237 - val_acc: 0.6333\n",
      "Epoch 91/100\n",
      "27/27 [==============================] - 27s 997ms/step - loss: 0.1995 - acc: 0.9370 - val_loss: 1.3535 - val_acc: 0.6556\n",
      "Epoch 92/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.1934 - acc: 0.9519 - val_loss: 1.3655 - val_acc: 0.6556\n",
      "Epoch 93/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2169 - acc: 0.9222 - val_loss: 1.4604 - val_acc: 0.6556\n",
      "Epoch 94/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2334 - acc: 0.9407 - val_loss: 1.4012 - val_acc: 0.6444\n",
      "Epoch 95/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.2858 - acc: 0.9037 - val_loss: 1.6531 - val_acc: 0.4889\n",
      "Epoch 96/100\n",
      "27/27 [==============================] - 27s 998ms/step - loss: 0.8976 - acc: 0.6741 - val_loss: 1.5583 - val_acc: 0.4889\n",
      "Epoch 97/100\n",
      "27/27 [==============================] - 26s 981ms/step - loss: 0.8095 - acc: 0.6852 - val_loss: 1.4787 - val_acc: 0.5333\n",
      "Epoch 98/100\n",
      "27/27 [==============================] - 27s 1s/step - loss: 0.6571 - acc: 0.7444 - val_loss: 1.4687 - val_acc: 0.5444\n",
      "Epoch 99/100\n",
      "27/27 [==============================] - 27s 997ms/step - loss: 0.5175 - acc: 0.8000 - val_loss: 1.4303 - val_acc: 0.5667\n",
      "Epoch 100/100\n",
      "27/27 [==============================] - 27s 997ms/step - loss: 0.5308 - acc: 0.7556 - val_loss: 1.4370 - val_acc: 0.5222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x220d6505c88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          epochs=NUM_EPOCHS, \n",
    "          steps_per_epoch=PER_EPOCH_STEPS, \n",
    "          validation_data=test_dataset, \n",
    "          validation_steps=TEST_PER_EPOCH_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wave",
   "language": "python",
   "name": "wave"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
